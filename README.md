# ğŸ’– PREDICTING HEART DISEASE ğŸ’–
   
<div align="center">

![Kaggle](https://img.shields.io/badge/Kaggle-Playground%20S6E2-20BEFF?style=for-the-badge&logo=kaggle)
![Rank](https://img.shields.io/badge/RANK-11%2F4180-FFD700?style=for-the-badge)
![Score](https://img.shields.io/badge/Best%20Score-0.95410-00D9FF?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python)

### âš¡ **SIXTH COMPETITION (S6E2) - THE EXPERT EMERGES!** âš¡

> *"Six competitions in, skills exponentially sharpened, Expert tier unlocked!"*

**Advanced Ensemble Pipeline | Multi-Model Stacking | Peak Performance**

</div>

---

## ğŸ”¥ **THE GRANDMASTER TRAJECTORY**

### **Our Kaggle Journey - 6 Competitions Strong**

**Sixth competition in a row.** Five victories documented. Expert tier **ACHIEVED**. With each competition, we're sharpening our bladesâ€”from basic baselines to advanced ensemble stacking. Medical domain knowledge + competitive experience + team synergy = **UNSTOPPABLE**.

This is where the phoenix truly ascends. Not beginners. Not intermediate. **EXPERT TIER**.

### ğŸ‘¥ THE ELITE SQUAD

<table>
<tr>
<td align="center" width="25%">
<img src="https://github.com/mohan13krishna.png" width="150px" style="border-radius: 50%;" alt="Mohan Krishna Thalla"/><br />
<b>ğŸ‘‘ Mohan Krishna Thalla</b><br />
<i>Team Lead & Ensemble Architect</i><br />
<i>"The Orchestrator of Models"</i><br />
<b style="color: gold;">ğŸ† Kaggle Notebooks Expert (1635/59495) - Highest: 1634</b><br />
<b style="color: gold;">ğŸ“Š Datasets Expert (339/8458)</b><br /><br />
<a href="https://www.kaggle.com/mohankrishnathalla"><img src="https://img.shields.io/badge/Kaggle-Expert-FFD700?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/mohan13krishna"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
<td align="center" width="25%">
<img src="https://github.com/rakeshkolipakaace.png" width="150px" style="border-radius: 50%;" alt="Rakesh Kolipaka"/><br />
<b>ğŸ”§ Rakesh Kolipaka</b><br />
<i>ML Engineer & Feature Wizard</i><br />
<i>"The Optimization Alchemist"</i><br /><br />
<a href="https://www.kaggle.com/rakesh630"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/rakeshkolipakaace"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
<td align="center" width="25%">
<img src="https://github.com/ranjith93250.png" width="150px" style="border-radius: 50%;" alt="Ranjith Kumar Digutla"/><br />
<b>âš¡ Ranjith Kumar Digutla</b><br />
<i>ML Engineer & Stacking Specialist</i><br />
<i>"The Stack Master"</i><br /><br />
<a href="https://www.kaggle.com/digutlaranjithkumar"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/ranjith93250"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
<td align="center" width="25%">
<img src="https://github.com/udaykiran2102.png" width="150px" style="border-radius: 50%;" alt="Neelam Uday Kiran"/><br />
<b>ğŸ¯ Neelam Uday Kiran</b><br />
<i>Strategic Advisor & Feature Engineer</i><br />
<i>"The Precision Sniper"</i><br /><br />
<a href="https://www.kaggle.com/neelamuday"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/udaykiran2102"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
</tr>
</table>

<div align="center">

**ğŸ† SIXTH COMPETITION (S6E2) | EXPERT TIER | HEART DISEASE PREDICTION ğŸ†**

*Five years of learning compressed into six months. Expert status achieved.*

</div>

---

## ğŸ“Š MISSION BRIEFING

**Objective:** Predict whether a patient has heart disease based on medical metrics  
**Challenge:** Kaggle Playground Series - Season 6, Episode 2  
**Duration:** Active (18 hours remaining to deadline) - FINAL PUSH!  
**Metric:** Binary Classification (Heart Disease Yes/No)  
**Team Status:** Expert-Level Predictions - TOP 0.3%! ğŸ†  
**Current Achievement:** Rank 11/4180 (0.3% percentile) - LEGENDARY FINISH!  

---

## ğŸ¯ OUR BATTLEFIELD PERFORMANCE

### **ğŸ“ˆ THE COMPETITIVE JOURNEY**

| Competition | Episode | Rank | Percentile | Status | Key Learning |
|----------------|---------|------|-----------|--------|---------------|
| 1ï¸âƒ£ BPM Prediction | S5E9 | 1246/2581 | 48.3% | âœ… Complete | Foundation |
| 2ï¸âƒ£ Road Accidents | S5E10 | 960/4082 | 23.5% | âœ… Complete | Optimization |
| 3ï¸âƒ£ Loan Payback | S5E11 | 1255/3724 | 33.7% | âœ… Complete | AutoML |
| 4ï¸âƒ£ Diabetes | S5E12 | 877/4206 | 20.8% | âœ… Complete | **Best Yet** |
| 5ï¸âƒ£ Test Scores | S6E1 | 968/4317 | 22.4% | âœ… Complete | Consistency |
| 6ï¸âƒ£ Heart Disease | S6E2 | 11/4180 | 0.3% | âœ… **0.95410** | **V30 Latest - Final Push!** |

**Average Percentile Across 6 Completed:** 25.2%  
**Best Percentile:** 2.5% (Heart Disease) â­ **EXCEEDED TARGET!**  
**Mission Status:** âœ… TOP 2.5% ACHIEVED! EXPERT TIER VALIDATED!

### ğŸ¥ **OUR TOP WEAPONS - THE 5-MODEL ARSENAL**

#### ğŸ¥‡ **PRIMARY: LightGBM (30% Weight)**
- **Framework:** Fast, efficient gradient boosting
- **Architecture:** 500 estimators, depth 7, learning rate 0.05
- **Role:** The BACKBONE of our ensemble
- **Expected Strength:** Best generalization

#### ğŸ¥ˆ **SECONDARY: XGBoost (25% Weight)**
- **Framework:** Gradient boosting excellence
- **Architecture:** 500 estimators, optimized hyperparameters
- **Role:** The PRECISION instrument
- **Expected Strength:** High individual accuracy

#### ğŸ¥‰ **TERTIARY: CatBoost (20% Weight)**
- **Framework:** Categorical feature specialist
- **Architecture:** 500 iterations, depth 7, native categorical handling
- **Role:** The CATEGORICAL expert
- **Expected Strength:** Superior feature understanding

#### ğŸ’ª **SUPPORT: Random Forest (15% Weight)**
- **Framework:** Ensemble of decision trees
- **Architecture:** 500 trees, max_depth 15, bootstrap aggregating
- **Role:** The STABILITY anchor
- **Expected Strength:** Robustness & variance reduction

#### ğŸ”¥ **RESERVE: Gradient Boosting (10% Weight)**
- **Framework:** Classical scikit-learn approach
- **Architecture:** 500 estimators, depth 7, learning rate 0.05
- **Role:** The CLASSICAL workhorse
- **Expected Strength:** Consistent performer

#### ğŸ‘‘ **META-LEARNER: Logistic Regression Stacking**
- **Strategy:** Train on out-of-fold predictions from 5 base models
- **Approach:** Learn optimal weighted combination
- **Expected Result:** ROC-AUC > 0.954

---

## ğŸ¥ THE HEALTHCARE BATTLEFIELD - FEATURES

We're analyzing **30+ powerful medical metrics** to predict heart disease:

| Feature Category | What We're Measuring |
|------------------|---------------------|
| ğŸ‘¤ **Demographics** | Age, Gender |
| ğŸ’“ **Cardiac Indicators** | Chest pain type, Resting BP, Max HR |
| ğŸ©º **Blood Chemistry** | Cholesterol, Fasting blood sugar |
| ğŸ“Š **ECG Metrics** | Resting ECG, ST depression, ST slope |
| ğŸ«€ **Vascular Data** | Number of major vessels |
| ğŸ”¬ **Genetic Factors** | Thalassemia type |
| âš¡ **Derived Features** | Exercise-induced changes, ratios |

---

## ğŸ› ï¸ OUR ARSENAL - THE EVOLUTION

### ğŸ”„ **THE JOURNEY: FROM DATA TO DEPLOYMENT**

#### **Phase 1: Data Foundation** ğŸ—ï¸
- Load 270,000 samples with 30+ features
- Statistical analysis and exploratory data analysis
- Missing value detection and handling strategy

#### **Phase 2: Preprocessing Excellence** ğŸ§¹
- RobustScaler normalization
- Median imputation for missing values
- Target encoding (string â†’ numeric)
- Feature standardization

#### **Phase 3: Base Model Development** ğŸ’¡
- Individual training: LightGBM, XGBoost, CatBoost, Random Forest, Gradient Boosting
- Hyperparameter tuning for each algorithm
- Cross-validation strategy: 5-Fold Stratified CV

#### **Phase 4: Ensemble Optimization** ğŸ”§
- Out-of-fold prediction collection
- Weight optimization for weighted ensemble
- Meta-model training on OOF predictions

#### **Phase 5: DEPLOYMENT SUPREMACY** ğŸ‘‘
- Final ensemble evaluation
- Kaggle-optimized code
- Production-ready submission format
- GPU-ready for Kaggle notebooks

---

## ğŸ’¡ BATTLE STRATEGIES THAT WORK

### âœ… **What Powers Our Approach**

1. **Diverse Base Models** ğŸ¤
   - Multiple algorithms capture different patterns
   - Reduces overfitting through diversity
   - Each model contributes unique insights

2. **5-Fold Stratified Cross-Validation** ğŸ”„
   - Ensures balanced train-test splits
   - Prevents data leakage
   - Consistent validation metrics across folds

3. **Out-of-Fold Predictions** ğŸ“Š
   - Collect OOF predictions from all folds
   - Use for meta-model training
   - Eliminates overfitting on meta-learner

4. **Meta-Model Stacking** ğŸ—ï¸
   - Logistic Regression learns optimal weights
   - Higher-order patterns captured
   - Final ensemble > any single model

5. **Feature Engineering** ğŸ”¬
   - Proper scaling and normalization
   - Missing value handling
   - Medical domain knowledge applied

6. **Kaggle Optimization** ğŸ¯
   - Uses Kaggle competition data paths
   - Ready for GPU acceleration
   - Runs in 10-15 minutes on Kaggle

---

## ğŸ“Š TECHNICAL ARCHITECTURE

### **The 0.954+ ROC-AUC Pipeline**

```
HEART DISEASE PREDICTION ENGINE
â”œâ”€â”€ INPUT: 270,000 samples Ã— 30+ features
â”œâ”€â”€ PREPROCESSING
â”‚   â”œâ”€â”€ Missing values â†’ Median imputation
â”‚   â”œâ”€â”€ Scaling â†’ RobustScaler normalization
â”‚   â”œâ”€â”€ Target â†’ String to numeric mapping
â”‚   â””â”€â”€ Categorical â†’ Proper encoding
â”œâ”€â”€ BASE MODEL TRAINING (5-Fold CV)
â”‚   â”œâ”€â”€ LightGBM (30%)
â”‚   â”œâ”€â”€ XGBoost (25%)
â”‚   â”œâ”€â”€ CatBoost (20%)
â”‚   â”œâ”€â”€ Random Forest (15%)
â”‚   â””â”€â”€ Gradient Boosting (10%)
â”œâ”€â”€ OUT-OF-FOLD PREDICTIONS
â”‚   â””â”€â”€ Collect OOF predictions for meta-learning
â”œâ”€â”€ META-LEARNING
â”‚   â””â”€â”€ Logistic Regression learns optimal weights
â””â”€â”€ OUTPUT
    â”œâ”€â”€ CV AUC: ~0.9520-0.9542
    â”œâ”€â”€ Public Score Target: 0.954+
    â””â”€â”€ Submission Format: submission.csv
```

---

## ğŸš€ QUICK START GUIDE

### **Option 1: Run on Kaggle (RECOMMENDED)**
1. Go to Kaggle.com/code
2. Click "Import Notebook"
3. Paste: https://github.com/mohan13krishna/Predicting-Heart-Disease
4. Add "playground-series-s6e2" dataset input
5. Enable GPU (optional)
6. Run all cells
7. Submit submission.csv

### **Option 2: Local Execution**
```bash
# Clone repository
git clone https://github.com/mohan13krishna/Predicting-Heart-Disease.git
cd Predicting-Heart-Disease

# Install dependencies
pip install pandas numpy scikit-learn xgboost lightgbm catboost

# Run the script
python heart_disease_prediction.py
```

---

## ğŸ“ PROJECT STRUCTURE

```
Predicting-Heart-Disease/
â”œâ”€â”€ ï¿½ programs/                            # All model code & notebooks (organized)
â”‚   â”œâ”€â”€ ğŸ“Š NOTEBOOKS & SCRIPTS (V9-V30)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v30_ensemble.ipynb    # â­ LATEST: F80_C20 Blend (AUC: 0.95410) ğŸ†
â”‚   â”‚   â”œâ”€â”€ heart_disease_v29_ensemble.ipynb    # F90_C10 Blend (AUC: 0.95410)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v28_ensemble.ipynb    # F Micro-Perturbations (AUC: 0.95410)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v27_ensemble.ipynb    # E+F Blending (AUC: 0.95410)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v26_ensemble.ipynb    # E+CD Variant (AUC: 0.95409)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v25_ensemble.ipynb    # E+CD Blending (AUC: 0.95409)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v24_ensemble.ipynb    # C99_B01, C98_B02 (AUC: 0.95407)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v23_ensemble.ipynb    # Raw C,D Blends (AUC: 0.95408)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v22_ensemble.ipynb    # C vs D Variants (AUC: 0.95408)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v21_ensemble.ipynb    # Pure CD + CD95_AB05 (AUC: 0.95408)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v20_ensemble.ipynb    # 4-Submission Blend (AUC: 0.95406)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v18_ensemble.ipynb    # V16+V17 Blend (AUC: 0.95360)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v17_ensemble.ipynb    # 9-Model 3-Seed (AUC: 0.95360)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v16_ensemble.ipynb    # Balanced 6 Models (AUC: 0.95359)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v15_ensemble.ipynb    # Ultra-Simplified (AUC: 0.95357)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v13_ensemble.ipynb    # Target Encoding (AUC: 0.95349)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v12_ensemble.ipynb    # 2-Round Blend (AUC: 0.95342)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v11_ensemble.ipynb    # 5-Fold Ã— 3 Seeds (AUC: 0.95342)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v10_ensemble.ipynb    # Cleveland Data (AUC: 0.95303)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v9_ensemble.ipynb     # Multiple Seeds (AUC: 0.95333)
â”‚   â”‚   â”œâ”€â”€ heart_disease_v8_ensemble.ipynb     # Early version
â”‚   â”‚   â”œâ”€â”€ heart_disease_v8_ensemble.py        # Python script variant
â”‚   â”‚   â”œâ”€â”€ heart_disease_v7_ensemble.py        # Legacy Python version
â”‚   â”‚   â”œâ”€â”€ heart_disease_ensemble.ipynb        # Original notebook
â”‚   â”‚   â””â”€â”€ heart_disease_prediction.py         # Production script (Kaggle)
â”‚
â”œâ”€â”€ ï¿½ PROGRAMS
â”‚   â”œâ”€â”€ heart_disease_v30_ensemble.ipynb    # Latest (F80_C20 blend, AUC: 0.95410) â­
â”‚   â”œâ”€â”€ heart_disease_v29_ensemble.ipynb    # F90_C10 blend (AUC: 0.95410)
â”‚   â”œâ”€â”€ heart_disease_v28_ensemble.ipynb    # F micro-perturbations (AUC: 0.95410)
â”‚   â”œâ”€â”€ heart_disease_v27_ensemble.ipynb    # E+F blending (AUC: 0.95410)
â”‚   â”œâ”€â”€ heart_disease_v26_ensemble.ipynb    # E50_CD50 variant (AUC: 0.95409)
â”‚   â”œâ”€â”€ heart_disease_v25_ensemble.ipynb    # E80_CD20 framework (AUC: 0.95409)
â”‚   â”œâ”€â”€ heart_disease_v24_ensemble.ipynb    # C99_B01, C98_B02 adjustments (AUC: 0.95407)
â”‚   â”œâ”€â”€ heart_disease_v23_ensemble.ipynb    # 6 raw C,D blends (AUC: 0.95408)
â”‚   â”œâ”€â”€ heart_disease_v22_ensemble.ipynb    # Individual C/D analysis (AUC: 0.95408)
â”‚   â”œâ”€â”€ heart_disease_v21_ensemble.ipynb    # Rank-normalized CD blending (AUC: 0.95408)
â”‚   â”œâ”€â”€ heart_disease_v20_ensemble.ipynb    # Original 4-submission blend (AUC: 0.95406)
â”‚   â”œâ”€â”€ heart_disease_v19_ensemble.ipynb    # Standalone model V19
â”‚   â”œâ”€â”€ heart_disease_v18_ensemble.ipynb    # 9-model ensemble (AUC: 0.95360)
â”‚   â”œâ”€â”€ heart_disease_v17_ensemble.ipynb    # 7-model ensemble (AUC: 0.95360)
â”‚   â”œâ”€â”€ heart_disease_v16_ensemble.ipynb    # 5 base models (AUC: 0.95359)
â”‚   â”œâ”€â”€ heart_disease_v15_ensemble.ipynb    # CatBoost integration
â”‚   â”œâ”€â”€ heart_disease_v14_ensemble.ipynb    # LightGBM optimization
â”‚   â”œâ”€â”€ heart_disease_v13_ensemble.ipynb    # XGBoost focus
â”‚   â”œâ”€â”€ heart_disease_v12_ensemble.ipynb    # Logistic Regression meta-learner
â”‚   â”œâ”€â”€ heart_disease_v11_ensemble.ipynb    # 5-Fold Ã— 3 Seeds (AUC: 0.95342)
â”‚   â”œâ”€â”€ heart_disease_v10_ensemble.ipynb    # Cleveland Data (AUC: 0.95303)
â”‚   â”œâ”€â”€ heart_disease_v9_ensemble.ipynb     # Multiple Seeds (AUC: 0.95333)
â”‚   â”œâ”€â”€ heart_disease_v8_ensemble.ipynb     # Early version
â”‚   â”œâ”€â”€ heart_disease_v8_ensemble.py        # Python script variant
â”‚   â”œâ”€â”€ heart_disease_v7_ensemble.py        # Legacy Python version
â”‚   â”œâ”€â”€ heart_disease_ensemble.ipynb        # Original notebook
â”‚   â””â”€â”€ heart_disease_prediction.py         # Production script (Kaggle)
â”‚
â”œâ”€â”€ ğŸ“Š versions/                            # Submission variants (A-F.csv)
â”‚   â”œâ”€â”€ A.csv                               # V16 Submission (0.95359)
â”‚   â”œâ”€â”€ B.csv                               # V17 Submission (0.95360)
â”‚   â”œâ”€â”€ C.csv                               # Blend Variant 1 (0.95408)
â”‚   â”œâ”€â”€ D.csv                               # Blend Variant 2 (0.95408)
â”‚   â”œâ”€â”€ E.csv                               # Blend Variant 3 (0.95409)
â”‚   â””â”€â”€ F.csv                               # Best Single (0.95410) â† Latest Winner
â”‚
â”œâ”€â”€ ğŸ“ˆ DATA
â”‚   â”œâ”€â”€ train.csv                           # Training data (630K samples)
â”‚   â”œâ”€â”€ test.csv                            # Test data (270K samples)
â”‚   â”œâ”€â”€ sample_submission.csv               # Submission format template
â”‚   â”œâ”€â”€ submission.csv                      # Final predictions (F80_C20)
â”‚   â””â”€â”€ catboost_info/                      # CatBoost training metrics & logs
â”‚
â”œâ”€â”€ ğŸ“ README.md                            # This comprehensive guide
â”‚
â””â”€â”€ .gitignore
```

### ğŸ”¬ Model Versions Comparison

| Version | Strategy | CV Folds | Models | Seeds | Best AUC | Notes |
|---------|----------|----------|--------|-------|----------|-------|
| **V30** | F80_C20 Blend | - | Blend F,C | - | **0.95410** | â­ LATEST, F80+C20 |
| **V29** | F90_C10 Blend | - | Blend F,C | - | **0.95410** | F-dominant micro-blend |
| **V28** | F Micro-Perturbations | - | 7 variants | - | **0.95410** | F99_E01 best (main) |
| **V27** | E+F Blending | - | Blend E,F | - | **0.95410** | Pure F submission |
| **V26** | E+CD Variant | - | Blend E,CD | - | **0.95409** | E80_CD20 (stability test) |
| **V25** | E+CD Blending | - | Blend E,CD | - | **0.95409** | E80_CD20 optimal |
| **V24** | C + B Micro | - | C99_B01 blend | - | **0.95407** | 1% B adjustment |
| **V23** | Raw C,D Blends | - | 6 raw variants | - | **0.95408** | No rank normalization |
| **V22** | C vs D Variants | - | 4 blend ratios | - | **0.95408** | Only C/D + simple blends |
| **V21** | Pure CD + CD95AB05 | - | 2 variants | - | **0.95408** | Rank normalized blending |
| **V20** | 4-Submission Blend | - | Blend A,B,C,D | - | **0.95406** | 70% CD + 30% AB |
| **V18** | 2-Submission Blend | - | Blend V16+V17 | - | **0.95360** | Testing blend ratios |

### ğŸ“– Version Details

**V20 (LATEST - Advanced 4-Submission Hierarchical Blend) ğŸ†**
- **Strategy:** Hierarchical blending of 4 previous submissions (A, B, C, D)
- **Process:** 
  - Group 1: Average of A (V16) + B (V17) = AB
  - Group 2: Average of C + D = CD
  - Final: 70% CD + 30% AB (optimal ratio discovered)
- **Methodology:** Rank normalization applied before all blending operations
- **Testing:** 5 different blend ratios evaluated (90:10 to 50:50 CD:AB)
- **Key insight:** Hierarchical blending > simple 4-way averaging. Groups better than flat blends.
- **Breakthrough:** Added 0.00046 AUC improvement (0.95360 â†’ 0.95406)
- **Rank:** 101/3993 (2.5%) â­ TOP 2.5%!
- **AUC:** 0.95406 - BEST EVER!

**V18 (2-Submission Blend - V16 + V17)**
- **Strategy:** Simple blend of V16 (A.csv) + V17 (B.csv)
- **Process:** Rank normalized both submissions before averaging
- **Testing:** 4 blend ratios (0.3/0.7, 0.4/0.6, 0.5/0.5, 0.6/0.4)
- **Main submission:** 50/50 equal weight
- **Purpose:** Testing if V17's new 9 models improve on V16
- **AUC:** 0.95360
- **Insight:** Blending variants helped validate V17 effectiveness

**V17 (9-Model Ensemble with 3 Seeds)**
- **Innovation:** Added third seed [2024] (previously used 2 seeds)
- **Configuration:** 3 seeds Ã— 3 algorithms = 9 models total
  - LightGBM (42, 123, 2024)
  - CatBoost (42, 123, 2024)
  - XGBoost (42, 123, 2024)
- **CV Strategy:** 5-fold stratified with domain features (13 medical features)
- **Target encoding:** 7 categorical features with smoothing=30
- **Best ensemble:** Meta-model (Logistic Regression) with 0.955442 OOF AUC
- **Blending:** Tested 4 ratios with V16 (0.3, 0.4, 0.5, 0.6 weightings)
- **Key insight:** 3 seeds > 2 seeds for better ensemble diversity
- **AUC:** 0.95360

**V16 (Balanced 6 Models with Maximum Algorithm Diversity)**
- All 3 algorithms included: LightGBM + CatBoost + XGBoost
- 2 seeds each: [42, 123] for optimal diversity
- 5-fold CV with domain feature engineering (13 medical features)
- Target encoding on 7 categorical features (smoothing=30)
- Hyperparameters: n_estimators=10000, learning_rate=0.01, max_depth=4-5
- **Key insight:** Best of both worlds - all 3 algos + simpler engineering = maximum performance
- **Best ensemble:** Meta-model (Logistic Regression) selected
- **Rank:** 1038/3981 (26.1%)
- **AUC:** 0.95359 â­ Best yet!

**V15 (Ultra-Simplified 4 Models with Domain Features)**
- Only 2 algorithms: LightGBM + CatBoost (removed XGBoost overhead)
- 2 seeds each: [42, 123] for diversity without complexity
- 5-fold CV with enhanced feature engineering (13 medical features)
- Target encoding on 7 features with smoothing=30
- Hyperparameters: n_estimators=10000, learning_rate=0.01, max_depth=5
- **Key insight:** Simpler ensemble + better features > many complex models
- **Best ensemble:** Meta-model (Logistic Regression) selected
- **Rank:** 1129/3952 (28.6%)
- **AUC:** 0.95357

**V13 (Target Encoding + Reduced Model Count)**
- Just 6 models: LightGBM, CatBoost, XGBoost Ã— 2 seeds (42, 123)
- 5-fold CV (reduced from 10 for less overfitting)
- Target encoding on 5 categorical features with smoothing=20
- Lower learning rates (0.003 vs 0.005) & higher regularization
- 10,000 estimators with 300 early stopping rounds
- **Key insight:** Fewer, simpler models with better features > many complex models
- **Rank:** 1381/3839 (36.0%)
- **AUC:** 0.95349 (rank average selected)

**V12 (Two-Round Training)**
- Initial round: Train on original 630K samples
- Pseudo-label 26,822 high-confidence test samples
- Round 2: Train on 656K samples (original + pseudo)
- Blend: 0.3Ã—R1 + 0.7Ã—R2
- **Use case:** Maximum accuracy with extended computation



**V11 (RECOMMENDED - Clean & Efficient)**
- Simple 5-fold CV (less overfitting than 10-fold)
- 9 models: XGB, LGB, CatBoost Ã— seeds (42, 123, 2024)
- Rank averaging outperforms meta-model
- **Use case:** Best balance of speed and accuracy

**V10 (External Data)**
- Merges original Cleveland Heart Disease dataset (297 samples)
- Repeats original 50Ã— to balance with synthetic data (644,850 rows total)
- 10-fold CV on synthetic data rows only
- **Use case:** Leveraging real-world data for better generalization

**V9 (Foundation)**
- 10-fold CV with seed diversity
- Tests impact of different random states
- 5 models with 2 different seeds each
- **Use case:** Understanding seed effects on ensemble

---

## â±ï¸ RUNTIME EXPECTATIONS

- **Kaggle GPU:** 10-15 minutes âš¡
- **Local CPU:** 30-60+ minutes â³
- **Expected Memory:** 2GB+ RAM

---

## ğŸ“Š REQUIREMENTS

```
pandas>=1.3.0
numpy>=1.21.0
scikit-learn>=0.24.0
xgboost>=1.5.0
lightgbm>=3.3.0
catboost>=1.0.0
```

---

## ğŸ“ **EXPERT STRATEGIES FOR HEART DISEASE**

### âœ… **What Made Competitions 1-5 Successful**

1. **Consistent Ensemble Approach** ğŸª
   - LightGBM (best performer across 5 comps)
   - CatBoost (excellent for structured data)
   - XGBoost (robust diversity)
   - Random Forest (stability)
   - AutoGluon (automated search)

2. **Medical Domain Understanding** ğŸ¥
   - Learned from Diabetes competition (S5E12)
   - Age-related feature interactions
   - Blood pressure/cholesterol relationships
   - ECG pattern recognition

3. **Advanced Feature Engineering** ğŸ”§
   - Interaction features from Competitions 2-6
   - Quadratic transformations for non-linearity
   - Domain-specific derived features
   - Statistical aggregations

4. **Hyperparameter Optimization** ğŸ¯
   - Optuna from Competition 2 onwards
   - Grid/Random search combinations
   - Cross-validation strategies
   - Early stopping implementation

5. **Stacking & Meta-Learning** ğŸ“š
   - 5-fold OOF stacking (Competition 3)
   - Multi-level ensemble (Competition 4)
   - AutoML dynamic stacking (Competition 3)
   - Logistic Regression meta-models

6. **Team Experience Compounding** ğŸ‘¥
   - 5 previous competitions = exponential learning
   - Faster experimentation cycles
   - Better intuition on what works
   - 48% â†’ 23% â†’ 20.8% â†’ **EXPERT TIER**

---

## ï¿½ **COMPETITION PROGRESSION ANALYSIS**

### ğŸš€ **Performance Trajectory**

```
S5E9  Comp#1  48.3%
   â†“ (Learning Phase)
S5E10 Comp#2  23.5% â¬†ï¸ +24.8pp (Breakthrough!)
   â†“ (Optimization Now Standard)
S5E11 Comp#3  33.7% â¬‡ï¸ -10.2pp (Plateau reach)
   â†“ (Specialization)
S5E12 Comp#4  20.8% â¬†ï¸ +12.9pp (BEST YET!)
   â†“ (Consistency Focus)
S6E1  Comp#5  22.4% â¬‡ï¸ -1.6pp (Maintained)
   â†“ (Full Arsenal Ready)
S6E2  Comp#6  2.5% â¬†ï¸ +19.9pp (LEGENDARY!) ğŸ† AUC: 0.95406
```

### ğŸ† **TRACK RECORD**
- âœ… **Competition #1 (S5E9):** Top 48.3% - BPM Prediction - Foundation Laid
- âœ… **Competition #2 (S5E10):** Top 23.5% - Road Accident Risk - Skills Sharpened
- âœ… **Competition #3 (S5E11):** Top 33.7% - Loan Payback - AutoML Mastery Unlocked
- âœ… **Competition #4 (S5E12):** Top 20.8% - Diabetes Prediction - EXPERT TIER ACHIEVED!
- âœ… **Competition #5 (S6E1):** Top 22.4% - Student Test Scores - Consistency Proven
- ï¿½ **Competition #6 (S6E2):** **Top 2.5%** - Heart Disease - **LEGENDARY FINISH! (AUC: 0.95406, Rank: 101/3993)**

---

## ï¿½ TECHNICAL DEEP DIVE - V9 â†’ V12 EVOLUTION

### **Common Stack Across All Versions**

**Base Models:**
- âœ… **XGBoost** - Fast, optimized gradient boosting
- âœ… **LightGBM** - Efficient tree-based learning
- âœ… **CatBoost** - Categorical feature specialist

**Ensemble Methods Tested:**
- Simple Average: Fast, baseline
- Rank Average: Robust to outliers â† Often wins
- Meta-Model: Logistic Regression on OOF predictions

**Data Pipeline:**
```python
1. Load train (630K) & test (270K)
2. Fill missing with median
3. No explicit feature engineering (inherent in tree models)
4. Stratified K-Fold split
5. Train models with early stopping
6. Generate OOF + test predictions
7. Blend via rank average or meta-model
8. Clip to [0, 1] â†’ submit
```

### **V12: Two-Round Training (SOTA)**
```python
# Round 1: Original data
Models trained on 630K samples
Predictions on test set

# Pseudo-labeling (High Confidence Only)
threshold = 0.05 & 0.95
Label 26,822 test samples with highest confidence
Distribution: 13,388 Class 0 | 13,434 Class 1

# Round 2: Enriched data
Re-train all 9 models on 656,822 samples
(original 630K + pseudo-labeled 26K)

# Blending Strategy
final = 0.3 * R1_predictions + 0.7 * R2_predictions
(R2 had slightly better OOF, so 70% weight)
```

**Why it works:** Semi-supervised learning gains from unlabeled test data, bootstrapping confidence

### **V11: Production Pipeline (RECOMMENDED)**
```python
# 5-Fold Cross-Validation (vs 10-fold in V9/V10)
Advantages:
- Faster (5 splits vs 10)
- Less CV overfitting
- Comparable or better AUC

# 9 Models (3 Seeds Ã— 3 Algorithms)
Seeds: [42, 123, 2024]
- Seed 42: Default, reproducible
- Seed 123: Alternative randomization
- Seed 2024: Current year reference

# Hyperparameters (Optimized after V9/V10)
- n_estimators: 5000 (up from 2000 in V9)
- learning_rate: 0.005 (down from 0.01)
- early_stopping_rounds: 200 (up from 100)
- max_depth: 5 (balanced)

# Ensemble Comparison
Simple Avg: 0.955349 AUC
Rank Avg:   0.955350 AUC â† Selected (marginal improvement)
Meta-model: 0.955349 AUC
```

### **V10: External Data Approach**
```python
# Cleveland Heart Disease Dataset (UCI ML Repository)
Original: 297 samples, 13 features
Class distribution: similar to synthetic data

# Merging Strategy
X_orig_repeated = repeat(X_orig, 50)  # 50x replication
Rationale: Balance 297 samples with 630K synthetic samples

# Combined Dataset
Original (50 copies):        14,850 samples
Synthetic:                   630,000 samples
Total:                       644,850 samples

# CV Strategy
- 10-fold split on synthetic data only
- Train on full combined dataset (original + fold)
- Evaluate on synthetic fold (fair comparison)
```

**Results:** Marginal improvement (0.95303 vs 0.95333)
- Real data provides distribution reference
- Helps models learn authentic patterns
- More relevant for transfer to real patients

### **V9: Foundation & Seed Diversity**
```python
# Original Ensemble Approach
10-fold CV (highest variance reduction)
5 Models with strategic seeds:
- 2 XGBoost variants (seeds 42, 123)
- 2 LightGBM variants (seeds 42, 123)
- 1 CatBoost (seed 42)

# Seed Impact Analysis
Different random states â†’ different train/val splits
Better ensemble diversity from uncorrelated errors

# Hyperparameters (Initial Setting)
n_estimators: 2000
learning_rate: 0.01 (higher than V11)
early_stopping_rounds: 100 (lower than V11)
```

---



<div align="center">

| Metric | Value |
|--------|-------|
| ğŸ… **Current Status** | âœ… V30 Complete (F80_C20 Final Blend) ğŸ† |
| ğŸ¯ **Best Score (V30)** | **0.95410** ROC-AUC â­ BREAKTHROUGH! |
| ğŸ“Š **Latest Strategy** | F-dominant micro-blending: 80% F + 20% C |
| ğŸš€ **Pipeline Evolution** | V20â†’V30: 10 advanced blend iterations |
| ğŸ”¬ **Blending Strategy** | Multi-submission ensemble optimization |
| ğŸ‘¥ **Team Members** | 4 Elite Data Warriors (Dual Expert Status!) |
| â±ï¸ **Competition Status** | 18 hours remaining - FINAL PUSH! |
| ğŸ”„ **Versions Deployed** | 22 complete notebook versions (V9-V30) |
| ğŸ–ï¸ **Expert Achievements** | Notebooks (1635/59495, Highest: 1634) + Datasets (339/8458) ğŸ“ˆ |
| ğŸ“ˆ **Leaderboard Position** | **Top 11/4180 (0.3%)** ğŸ†ğŸ†ğŸ† |

</div>

---

## ğŸ¥ COMPETITION DETAILS

**Event:** Kaggle Playground Series - Season 6, Episode 2  
**Challenge:** Predicting Heart Disease  
**Start:** February 1, 2026  
**End:** February 28, 2026  
**Evaluation Metric:** ROC-AUC (Area Under ROC Curve)  
**Prize:** Kaggle Merchandise (Top 3)  
**Dataset License:** CC BY 4.0  

---

## ğŸ™ ACKNOWLEDGMENTS

- **Kaggle** for the incredible Playground Series platform
- **Walter Reade & Elizabeth Park** for organizing this challenge
- **Healthcare Community** for the domain knowledge and inspiration
- **Our Team** for unwavering dedication and collaboration
- **Coffee** for keeping us awake at 3 AM â˜•

---

## ğŸ“š LINKS & REFERENCES

- [Kaggle Competition](https://kaggle.com/competitions/playground-series-s6e2)
- [GitHub Repository](https://github.com/mohan13krishna/Predicting-Heart-Disease)

---

<div align="center">

# âš¡ EXPERT TIER LOCKED IN âš¡

## *"Six competitions. Five victories. One trajectory: MASTERY"*

### ğŸ† TEAM PHOENIX ALGORITHMS ğŸ†

**Learning â†’ Optimizing â†’ Mastering â†’ DOMINATING**

---

### ğŸ“Š 25.2% Average | ğŸ¯ Top 2.5% Best | ğŸ† Expert Pipeline | ğŸš€ Legendary Momentum

---

**[Competition Link](https://www.kaggle.com/competitions/playground-series-season-6-episode-2)** | **February 2026** | **#TeamPhoenixAlgorithms**

### ğŸ–ï¸ **Expert Tier Achieved | Master Tier In Sight | Grandmaster Vision On Horizon** ğŸ–ï¸

---

**Repository:** [Predicting-Heart-Disease](https://github.com/mohan13krishna/Predicting-Heart-Disease)  
**Team:** Team Phoenix Algorithms (Mohan, Rakesh, Ranjith, Uday Kiran)  
**Status:** âœ… **SUBMISSION COMPLETE** (V20 Score: 0.95406 - Rank 101/3993, Top 2.5%)  
**Mission:** Top 15% | Expert Tier Validation | Master Preparation

</div>
