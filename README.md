# ğŸ’– PREDICTING HEART DISEASE ğŸ’–
   
<div align="center">

![Kaggle](https://img.shields.io/badge/Kaggle-Playground%20S6E2-20BEFF?style=for-the-badge&logo=kaggle)
![Rank](https://img.shields.io/badge/RANK-1129%2F3952-FFD700?style=for-the-badge)
![Score](https://img.shields.io/badge/Best%20Score-0.95357-00D9FF?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python)

### âš¡ **SIXTH COMPETITION (S6E2) - THE EXPERT EMERGES!** âš¡

> *"Six competitions in, skills exponentially sharpened, Expert tier unlocked!"*

**Advanced Ensemble Pipeline | Multi-Model Stacking | Peak Performance**

</div>

---

## ğŸ”¥ **THE GRANDMASTER TRAJECTORY**

### **Our Kaggle Journey - 6 Competitions Strong**

**Sixth competition in a row.** Five victories documented. Expert tier **ACHIEVED**. With each competition, we're sharpening our bladesâ€”from basic baselines to advanced ensemble stacking. Medical domain knowledge + competitive experience + team synergy = **UNSTOPPABLE**.

This is where the phoenix truly ascends. Not beginners. Not intermediate. **EXPERT TIER**.

### ğŸ‘¥ THE ELITE SQUAD

<table>
<tr>
<td align="center" width="25%">
<img src="https://github.com/mohan13krishna.png" width="150px" style="border-radius: 50%;" alt="Mohan Krishna Thalla"/><br />
<b>ğŸ‘‘ Mohan Krishna Thalla</b><br />
<i>Team Lead & Ensemble Architect</i><br />
<i>"The Orchestrator of Models"</i><br /><br />
<a href="https://www.kaggle.com/mohankrishnathalla"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/mohan13krishna"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
<td align="center" width="25%">
<img src="https://github.com/rakeshkolipakaace.png" width="150px" style="border-radius: 50%;" alt="Rakesh Kolipaka"/><br />
<b>ğŸ”§ Rakesh Kolipaka</b><br />
<i>ML Engineer & Feature Wizard</i><br />
<i>"The Optimization Alchemist"</i><br /><br />
<a href="https://www.kaggle.com/rakesh630"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/rakeshkolipakaace"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
<td align="center" width="25%">
<img src="https://github.com/ranjith93250.png" width="150px" style="border-radius: 50%;" alt="Ranjith Kumar Digutla"/><br />
<b>âš¡ Ranjith Kumar Digutla</b><br />
<i>ML Engineer & Stacking Specialist</i><br />
<i>"The Stack Master"</i><br /><br />
<a href="https://www.kaggle.com/digutlaranjithkumar"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/ranjith93250"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
<td align="center" width="25%">
<img src="https://github.com/udaykiran2102.png" width="150px" style="border-radius: 50%;" alt="Neelam Uday Kiran"/><br />
<b>ğŸ¯ Neelam Uday Kiran</b><br />
<i>Strategic Advisor & Feature Engineer</i><br />
<i>"The Precision Sniper"</i><br /><br />
<a href="https://www.kaggle.com/neelamuday"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/udaykiran2102"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
</tr>
</table>

<div align="center">

**ğŸ† SIXTH COMPETITION (S6E2) | EXPERT TIER | HEART DISEASE PREDICTION ğŸ†**

*Five years of learning compressed into six months. Expert status achieved.*

</div>

---

## ğŸ“Š MISSION BRIEFING

**Objective:** Predict whether a patient has heart disease based on medical metrics  
**Challenge:** Kaggle Playground Series - Season 6, Episode 2  
**Duration:** Active (5 days remaining to deadline)  
**Metric:** Binary Classification (Heart Disease Yes/No)  
**Team Status:** Expert-Level Predictions  
**Ultimate Goal:** Master Tier & Consistent Podium Finishes  

---

## ğŸ¯ OUR BATTLEFIELD PERFORMANCE

### **ğŸ“ˆ THE COMPETITIVE JOURNEY**

| Competition | Episode | Rank | Percentile | Status | Key Learning |
|----------------|---------|------|-----------|--------|---------------|
| 1ï¸âƒ£ BPM Prediction | S5E9 | 1246/2581 | 48.3% | âœ… Complete | Foundation |
| 2ï¸âƒ£ Road Accidents | S5E10 | 960/4082 | 23.5% | âœ… Complete | Optimization |
| 3ï¸âƒ£ Loan Payback | S5E11 | 1255/3724 | 33.7% | âœ… Complete | AutoML |
| 4ï¸âƒ£ Diabetes | S5E12 | 877/4206 | 20.8% | âœ… Complete | **Best Yet** |
| 5ï¸âƒ£ Test Scores | S6E1 | 968/4317 | 22.4% | âœ… Complete | Consistency |
| 6ï¸âƒ£ Heart Disease | S6E2 | 1129/3952 | 28.6% | âœ… **0.95357** | **V15 Latest** |

**Average Percentile Across 5 Completed:** 26.6%  
**Best Percentile:** 20.8% (Diabetes)  
**Current Mission:** Achieve Top 15% & Master Status

### ğŸ¥ **OUR TOP WEAPONS - THE 5-MODEL ARSENAL**

#### ğŸ¥‡ **PRIMARY: LightGBM (30% Weight)**
- **Framework:** Fast, efficient gradient boosting
- **Architecture:** 500 estimators, depth 7, learning rate 0.05
- **Role:** The BACKBONE of our ensemble
- **Expected Strength:** Best generalization

#### ğŸ¥ˆ **SECONDARY: XGBoost (25% Weight)**
- **Framework:** Gradient boosting excellence
- **Architecture:** 500 estimators, optimized hyperparameters
- **Role:** The PRECISION instrument
- **Expected Strength:** High individual accuracy

#### ğŸ¥‰ **TERTIARY: CatBoost (20% Weight)**
- **Framework:** Categorical feature specialist
- **Architecture:** 500 iterations, depth 7, native categorical handling
- **Role:** The CATEGORICAL expert
- **Expected Strength:** Superior feature understanding

#### ğŸ’ª **SUPPORT: Random Forest (15% Weight)**
- **Framework:** Ensemble of decision trees
- **Architecture:** 500 trees, max_depth 15, bootstrap aggregating
- **Role:** The STABILITY anchor
- **Expected Strength:** Robustness & variance reduction

#### ğŸ”¥ **RESERVE: Gradient Boosting (10% Weight)**
- **Framework:** Classical scikit-learn approach
- **Architecture:** 500 estimators, depth 7, learning rate 0.05
- **Role:** The CLASSICAL workhorse
- **Expected Strength:** Consistent performer

#### ğŸ‘‘ **META-LEARNER: Logistic Regression Stacking**
- **Strategy:** Train on out-of-fold predictions from 5 base models
- **Approach:** Learn optimal weighted combination
- **Expected Result:** ROC-AUC > 0.954

---

## ğŸ¥ THE HEALTHCARE BATTLEFIELD - FEATURES

We're analyzing **30+ powerful medical metrics** to predict heart disease:

| Feature Category | What We're Measuring |
|------------------|---------------------|
| ğŸ‘¤ **Demographics** | Age, Gender |
| ğŸ’“ **Cardiac Indicators** | Chest pain type, Resting BP, Max HR |
| ğŸ©º **Blood Chemistry** | Cholesterol, Fasting blood sugar |
| ğŸ“Š **ECG Metrics** | Resting ECG, ST depression, ST slope |
| ğŸ«€ **Vascular Data** | Number of major vessels |
| ğŸ”¬ **Genetic Factors** | Thalassemia type |
| âš¡ **Derived Features** | Exercise-induced changes, ratios |

---

## ğŸ› ï¸ OUR ARSENAL - THE EVOLUTION

### ğŸ”„ **THE JOURNEY: FROM DATA TO DEPLOYMENT**

#### **Phase 1: Data Foundation** ğŸ—ï¸
- Load 270,000 samples with 30+ features
- Statistical analysis and exploratory data analysis
- Missing value detection and handling strategy

#### **Phase 2: Preprocessing Excellence** ğŸ§¹
- RobustScaler normalization
- Median imputation for missing values
- Target encoding (string â†’ numeric)
- Feature standardization

#### **Phase 3: Base Model Development** ğŸ’¡
- Individual training: LightGBM, XGBoost, CatBoost, Random Forest, Gradient Boosting
- Hyperparameter tuning for each algorithm
- Cross-validation strategy: 5-Fold Stratified CV

#### **Phase 4: Ensemble Optimization** ğŸ”§
- Out-of-fold prediction collection
- Weight optimization for weighted ensemble
- Meta-model training on OOF predictions

#### **Phase 5: DEPLOYMENT SUPREMACY** ğŸ‘‘
- Final ensemble evaluation
- Kaggle-optimized code
- Production-ready submission format
- GPU-ready for Kaggle notebooks

---

## ğŸ’¡ BATTLE STRATEGIES THAT WORK

### âœ… **What Powers Our Approach**

1. **Diverse Base Models** ğŸ¤
   - Multiple algorithms capture different patterns
   - Reduces overfitting through diversity
   - Each model contributes unique insights

2. **5-Fold Stratified Cross-Validation** ğŸ”„
   - Ensures balanced train-test splits
   - Prevents data leakage
   - Consistent validation metrics across folds

3. **Out-of-Fold Predictions** ğŸ“Š
   - Collect OOF predictions from all folds
   - Use for meta-model training
   - Eliminates overfitting on meta-learner

4. **Meta-Model Stacking** ğŸ—ï¸
   - Logistic Regression learns optimal weights
   - Higher-order patterns captured
   - Final ensemble > any single model

5. **Feature Engineering** ğŸ”¬
   - Proper scaling and normalization
   - Missing value handling
   - Medical domain knowledge applied

6. **Kaggle Optimization** ğŸ¯
   - Uses Kaggle competition data paths
   - Ready for GPU acceleration
   - Runs in 10-15 minutes on Kaggle

---

## ğŸ“Š TECHNICAL ARCHITECTURE

### **The 0.954+ ROC-AUC Pipeline**

```
HEART DISEASE PREDICTION ENGINE
â”œâ”€â”€ INPUT: 270,000 samples Ã— 30+ features
â”œâ”€â”€ PREPROCESSING
â”‚   â”œâ”€â”€ Missing values â†’ Median imputation
â”‚   â”œâ”€â”€ Scaling â†’ RobustScaler normalization
â”‚   â”œâ”€â”€ Target â†’ String to numeric mapping
â”‚   â””â”€â”€ Categorical â†’ Proper encoding
â”œâ”€â”€ BASE MODEL TRAINING (5-Fold CV)
â”‚   â”œâ”€â”€ LightGBM (30%)
â”‚   â”œâ”€â”€ XGBoost (25%)
â”‚   â”œâ”€â”€ CatBoost (20%)
â”‚   â”œâ”€â”€ Random Forest (15%)
â”‚   â””â”€â”€ Gradient Boosting (10%)
â”œâ”€â”€ OUT-OF-FOLD PREDICTIONS
â”‚   â””â”€â”€ Collect OOF predictions for meta-learning
â”œâ”€â”€ META-LEARNING
â”‚   â””â”€â”€ Logistic Regression learns optimal weights
â””â”€â”€ OUTPUT
    â”œâ”€â”€ CV AUC: ~0.9520-0.9542
    â”œâ”€â”€ Public Score Target: 0.954+
    â””â”€â”€ Submission Format: submission.csv
```

---

## ğŸš€ QUICK START GUIDE

### **Option 1: Run on Kaggle (RECOMMENDED)**
1. Go to Kaggle.com/code
2. Click "Import Notebook"
3. Paste: https://github.com/mohan13krishna/Predicting-Heart-Disease
4. Add "playground-series-s6e2" dataset input
5. Enable GPU (optional)
6. Run all cells
7. Submit submission.csv

### **Option 2: Local Execution**
```bash
# Clone repository
git clone https://github.com/mohan13krishna/Predicting-Heart-Disease.git
cd Predicting-Heart-Disease

# Install dependencies
pip install pandas numpy scikit-learn xgboost lightgbm catboost

# Run the script
python heart_disease_prediction.py
```

---

## ğŸ“ PROJECT STRUCTURE

```
Predicting-Heart-Disease/
â”œâ”€â”€ ğŸ“Š NOTEBOOKS (Model Versions)
â”‚   â”œâ”€â”€ heart_disease_v15_ensemble.ipynb    # â­ LATEST: Ultra-Simplified 4 Models (AUC: 0.95357)
â”‚   â”œâ”€â”€ heart_disease_v13_ensemble.ipynb    # Target Encoding + 6 Models (AUC: 0.95349)
â”‚   â”œâ”€â”€ heart_disease_v12_ensemble.ipynb    # 2-Round + Pseudo-labeling (AUC: 0.95342)
â”‚   â”œâ”€â”€ heart_disease_v11_ensemble.ipynb    # Simplified 5-Fold Ã— 3 Seeds (AUC: 0.95342)
â”‚   â”œâ”€â”€ heart_disease_v10_ensemble.ipynb    # Cleveland Data Integration (AUC: 0.95303)
â”‚   â”œâ”€â”€ heart_disease_v9_ensemble.ipynb     # Multiple Seeds Ensemble (AUC: 0.95333)
â”‚   â”œâ”€â”€ heart_disease_ensemble.ipynb        # Original ensemble notebook
â”‚
â”œâ”€â”€ ğŸ’» SCRIPTS
â”‚   â”œâ”€â”€ heart_disease_prediction.py         # Production Python script (Kaggle)
â”‚   â”œâ”€â”€ heart_disease_v7_ensemble.py        # Earlier version script
â”‚
â”œâ”€â”€ ğŸ“ˆ DATA
â”‚   â”œâ”€â”€ train.csv                           # Training data (630K samples, 13 features)
â”‚   â”œâ”€â”€ test.csv                            # Test data (270K samples)
â”‚   â”œâ”€â”€ sample_submission.csv               # Submission format template
â”‚   â”œâ”€â”€ submission.csv                      # Final predictions (updated)
â”‚
â”œâ”€â”€ ğŸ“ DOCUMENTATION
â”‚   â””â”€â”€ README.md                           # This comprehensive guide
â”‚
â””â”€â”€ ğŸ“¦ CatBoost Logs
    â””â”€â”€ catboost_info/                      # Training metrics & logs
```

### ğŸ”¬ Model Versions Comparison

| Version | Strategy | CV Folds | Models | Seeds | Best AUC | Notes |
|---------|----------|----------|--------|-------|----------|-------|
| **V15** | Ultra-Simplified 4 Models | 5 | 4 (LGBÃ—2, CATÃ—2) | 2 | **0.95357** | â­ Latest, best yet! |  
| **V13** | Target Encoding + 6 Models | 5 | 6 (LGBÃ—2, CATÃ—2, XGBÃ—2) | 2 | **0.95349** | Less overfitting |  
| **V12** | 2-Round + Pseudo-labeling | 5 | 9 (XGB+LGB+CAT) | 3 | **0.95342** | Semi-supervised, advanced |
| **V11** | Simplified Ensemble | 5 | 9 (XGB+LGB+CAT) | 3 | **0.95342** | âœ… Recommended, clean code |
| **V10** | Cleveland Data Integration | 10 | 10 (Ã— 2-3 seeds) | - | **0.95303** | Real external dataset |
| **V9** | Multiple Seeds | 10 | 5 (XGBÃ—2, LGBÃ—2, CAT) | 2 | **0.95333** | Seed diversity focus |

### ğŸ“– Version Details

**V15 (LATEST - Ultra-Simplified 4 Models with Domain Features)**
- Only 2 algorithms: LightGBM + CatBoost (removed XGBoost overhead)
- 2 seeds each: [42, 123] for diversity without complexity
- 5-fold CV with enhanced feature engineering (13 medical features)
- Target encoding on 7 features with smoothing=30
- Hyperparameters: n_estimators=10000, learning_rate=0.01, max_depth=5
- **Key insight:** Simpler ensemble + better features > many complex models
- **Best ensemble:** Meta-model (Logistic Regression) selected
- **Rank:** 1129/3952 (28.6%)
- **AUC:** 0.95357 (new best!)

**V13 (Target Encoding + Reduced Model Count)**
- Just 6 models: LightGBM, CatBoost, XGBoost Ã— 2 seeds (42, 123)
- 5-fold CV (reduced from 10 for less overfitting)
- Target encoding on 5 categorical features with smoothing=20
- Lower learning rates (0.003 vs 0.005) & higher regularization
- 10,000 estimators with 300 early stopping rounds
- **Key insight:** Fewer, simpler models with better features > many complex models
- **Rank:** 1381/3839 (36.0%)
- **AUC:** 0.95349 (rank average selected)

**V12 (Two-Round Training)**
- Initial round: Train on original 630K samples
- Pseudo-label 26,822 high-confidence test samples
- Round 2: Train on 656K samples (original + pseudo)
- Blend: 0.3Ã—R1 + 0.7Ã—R2
- **Use case:** Maximum accuracy with extended computation



**V11 (RECOMMENDED - Clean & Efficient)**
- Simple 5-fold CV (less overfitting than 10-fold)
- 9 models: XGB, LGB, CatBoost Ã— seeds (42, 123, 2024)
- Rank averaging outperforms meta-model
- **Use case:** Best balance of speed and accuracy

**V10 (External Data)**
- Merges original Cleveland Heart Disease dataset (297 samples)
- Repeats original 50Ã— to balance with synthetic data (644,850 rows total)
- 10-fold CV on synthetic data rows only
- **Use case:** Leveraging real-world data for better generalization

**V9 (Foundation)**
- 10-fold CV with seed diversity
- Tests impact of different random states
- 5 models with 2 different seeds each
- **Use case:** Understanding seed effects on ensemble

---

## â±ï¸ RUNTIME EXPECTATIONS

- **Kaggle GPU:** 10-15 minutes âš¡
- **Local CPU:** 30-60+ minutes â³
- **Expected Memory:** 2GB+ RAM

---

## ğŸ“Š REQUIREMENTS

```
pandas>=1.3.0
numpy>=1.21.0
scikit-learn>=0.24.0
xgboost>=1.5.0
lightgbm>=3.3.0
catboost>=1.0.0
```

---

## ğŸ“ **EXPERT STRATEGIES FOR HEART DISEASE**

### âœ… **What Made Competitions 1-5 Successful**

1. **Consistent Ensemble Approach** ğŸª
   - LightGBM (best performer across 5 comps)
   - CatBoost (excellent for structured data)
   - XGBoost (robust diversity)
   - Random Forest (stability)
   - AutoGluon (automated search)

2. **Medical Domain Understanding** ğŸ¥
   - Learned from Diabetes competition (S5E12)
   - Age-related feature interactions
   - Blood pressure/cholesterol relationships
   - ECG pattern recognition

3. **Advanced Feature Engineering** ğŸ”§
   - Interaction features from Competitions 2-6
   - Quadratic transformations for non-linearity
   - Domain-specific derived features
   - Statistical aggregations

4. **Hyperparameter Optimization** ğŸ¯
   - Optuna from Competition 2 onwards
   - Grid/Random search combinations
   - Cross-validation strategies
   - Early stopping implementation

5. **Stacking & Meta-Learning** ğŸ“š
   - 5-fold OOF stacking (Competition 3)
   - Multi-level ensemble (Competition 4)
   - AutoML dynamic stacking (Competition 3)
   - Logistic Regression meta-models

6. **Team Experience Compounding** ğŸ‘¥
   - 5 previous competitions = exponential learning
   - Faster experimentation cycles
   - Better intuition on what works
   - 48% â†’ 23% â†’ 20.8% â†’ **EXPERT TIER**

---

## ï¿½ **COMPETITION PROGRESSION ANALYSIS**

### ğŸš€ **Performance Trajectory**

```
S5E9  Comp#1  48.3%
   â†“ (Learning Phase)
S5E10 Comp#2  23.5% â¬†ï¸ +24.8pp (Breakthrough!)
   â†“ (Optimization Now Standard)
S5E11 Comp#3  33.7% â¬‡ï¸ -10.2pp (Plateau reach)
   â†“ (Specialization)
S5E12 Comp#4  20.8% â¬†ï¸ +12.9pp (BEST YET!)
   â†“ (Consistency Focus)
S6E1  Comp#5  22.4% â¬‡ï¸ -1.6pp (Maintained)
   â†“ (Full Arsenal Ready)
S6E2  Comp#6  ???  ğŸ¯ (Target: Top 15%)
```

### ğŸ† **TRACK RECORD**
- âœ… **Competition #1 (S5E9):** Top 48.3% - BPM Prediction - Foundation Laid
- âœ… **Competition #2 (S5E10):** Top 23.5% - Road Accident Risk - Skills Sharpened
- âœ… **Competition #3 (S5E11):** Top 33.7% - Loan Payback - AutoML Mastery Unlocked
- âœ… **Competition #4 (S5E12):** Top 20.8% - Diabetes Prediction - EXPERT TIER ACHIEVED!
- âœ… **Competition #5 (S6E1):** Top 22.4% - Student Test Scores - Consistency Proven
- ğŸ¯ **Competition #6 (S6E2):** TBD - Heart Disease - **CURRENT MISSION (Active)**

---

## ï¿½ TECHNICAL DEEP DIVE - V9 â†’ V12 EVOLUTION

### **Common Stack Across All Versions**

**Base Models:**
- âœ… **XGBoost** - Fast, optimized gradient boosting
- âœ… **LightGBM** - Efficient tree-based learning
- âœ… **CatBoost** - Categorical feature specialist

**Ensemble Methods Tested:**
- Simple Average: Fast, baseline
- Rank Average: Robust to outliers â† Often wins
- Meta-Model: Logistic Regression on OOF predictions

**Data Pipeline:**
```python
1. Load train (630K) & test (270K)
2. Fill missing with median
3. No explicit feature engineering (inherent in tree models)
4. Stratified K-Fold split
5. Train models with early stopping
6. Generate OOF + test predictions
7. Blend via rank average or meta-model
8. Clip to [0, 1] â†’ submit
```

### **V12: Two-Round Training (SOTA)**
```python
# Round 1: Original data
Models trained on 630K samples
Predictions on test set

# Pseudo-labeling (High Confidence Only)
threshold = 0.05 & 0.95
Label 26,822 test samples with highest confidence
Distribution: 13,388 Class 0 | 13,434 Class 1

# Round 2: Enriched data
Re-train all 9 models on 656,822 samples
(original 630K + pseudo-labeled 26K)

# Blending Strategy
final = 0.3 * R1_predictions + 0.7 * R2_predictions
(R2 had slightly better OOF, so 70% weight)
```

**Why it works:** Semi-supervised learning gains from unlabeled test data, bootstrapping confidence

### **V11: Production Pipeline (RECOMMENDED)**
```python
# 5-Fold Cross-Validation (vs 10-fold in V9/V10)
Advantages:
- Faster (5 splits vs 10)
- Less CV overfitting
- Comparable or better AUC

# 9 Models (3 Seeds Ã— 3 Algorithms)
Seeds: [42, 123, 2024]
- Seed 42: Default, reproducible
- Seed 123: Alternative randomization
- Seed 2024: Current year reference

# Hyperparameters (Optimized after V9/V10)
- n_estimators: 5000 (up from 2000 in V9)
- learning_rate: 0.005 (down from 0.01)
- early_stopping_rounds: 200 (up from 100)
- max_depth: 5 (balanced)

# Ensemble Comparison
Simple Avg: 0.955349 AUC
Rank Avg:   0.955350 AUC â† Selected (marginal improvement)
Meta-model: 0.955349 AUC
```

### **V10: External Data Approach**
```python
# Cleveland Heart Disease Dataset (UCI ML Repository)
Original: 297 samples, 13 features
Class distribution: similar to synthetic data

# Merging Strategy
X_orig_repeated = repeat(X_orig, 50)  # 50x replication
Rationale: Balance 297 samples with 630K synthetic samples

# Combined Dataset
Original (50 copies):        14,850 samples
Synthetic:                   630,000 samples
Total:                       644,850 samples

# CV Strategy
- 10-fold split on synthetic data only
- Train on full combined dataset (original + fold)
- Evaluate on synthetic fold (fair comparison)
```

**Results:** Marginal improvement (0.95303 vs 0.95333)
- Real data provides distribution reference
- Helps models learn authentic patterns
- More relevant for transfer to real patients

### **V9: Foundation & Seed Diversity**
```python
# Original Ensemble Approach
10-fold CV (highest variance reduction)
5 Models with strategic seeds:
- 2 XGBoost variants (seeds 42, 123)
- 2 LightGBM variants (seeds 42, 123)
- 1 CatBoost (seed 42)

# Seed Impact Analysis
Different random states â†’ different train/val splits
Better ensemble diversity from uncorrelated errors

# Hyperparameters (Initial Setting)
n_estimators: 2000
learning_rate: 0.01 (higher than V11)
early_stopping_rounds: 100 (lower than V11)
```

---



<div align="center">

| Metric | Value |
|--------|-------|
| ğŸ… **Current Status** | âœ… V15 Complete (Ultra-Simplified) |
| ğŸ¯ **Best Score (V15)** | **0.95357** ROC-AUC |
| ğŸ“Š **Ensemble Models** | 4 (2 Seeds Ã— 2 Algorithms) |
| ğŸš€ **Training Samples** | 630K original samples |
| ğŸ”¬ **Features** | 30 (13 original + 13 engineered + 4 target-encoded) |
| ğŸ‘¥ **Team Members** | 4 Elite Data Warriors |
| â±ï¸ **Competition Duration** | 28 Days (Feb 1-28, 2026) |
| ğŸ”„ **Models Trained** | 4 Ã— 5-folds = 20 base models |
| â˜• **Coffee Consumed** | âˆ (Unlimited) |
| ğŸ“ˆ **Leaderboard Position** | Top 1129/3952 (28.6%) |

</div>

---

## ğŸ¥ COMPETITION DETAILS

**Event:** Kaggle Playground Series - Season 6, Episode 2  
**Challenge:** Predicting Heart Disease  
**Start:** February 1, 2026  
**End:** February 28, 2026  
**Evaluation Metric:** ROC-AUC (Area Under ROC Curve)  
**Prize:** Kaggle Merchandise (Top 3)  
**Dataset License:** CC BY 4.0  

---

## ğŸ™ ACKNOWLEDGMENTS

- **Kaggle** for the incredible Playground Series platform
- **Walter Reade & Elizabeth Park** for organizing this challenge
- **Healthcare Community** for the domain knowledge and inspiration
- **Our Team** for unwavering dedication and collaboration
- **Coffee** for keeping us awake at 3 AM â˜•

---

## ğŸ“š LINKS & REFERENCES

- [Kaggle Competition](https://kaggle.com/competitions/playground-series-s6e2)
- [GitHub Repository](https://github.com/mohan13krishna/Predicting-Heart-Disease)
- [Team Phoenix on Kaggle](https://kaggle.com/mohan13krishna)

---

<div align="center">

# âš¡ EXPERT TIER LOCKED IN âš¡

## *"Six competitions. Five victories. One trajectory: MASTERY"*

### ğŸ† TEAM PHOENIX ALGORITHMS ğŸ†

**Learning â†’ Optimizing â†’ Mastering â†’ DOMINATING**

---

### ğŸ“Š 26.6% Average | ğŸ¤– Expert Pipeline | ğŸš€ Unstoppable Momentum

---

**[Competition Link](https://www.kaggle.com/competitions/playground-series-season-6-episode-2)** | **February 2026** | **#TeamPhoenixAlgorithms**

### ğŸ–ï¸ **Expert Tier Achieved | Master Tier In Sight | Grandmaster Vision On Horizon** ğŸ–ï¸

---

**Repository:** [Predicting-Heart-Disease](https://github.com/mohan13krishna/Predicting-Heart-Disease)  
**Team:** Team Phoenix Algorithms (Mohan, Rakesh, Ranjith, Uday Kiran)  
**Status:** âœ… **SUBMISSION COMPLETE** (V7 Score: 0.95324)  
**Mission:** Top 15% | Expert Tier Validation | Master Preparation

</div>
