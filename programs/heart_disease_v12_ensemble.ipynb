{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdf06820",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction - V12\n",
    "## Two-Round Training with Pseudo-Labeling (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc63f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985e047",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71095a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "target_column = 'Heart Disease'\n",
    "X_train = train_df.drop([target_column, 'id'], axis=1)\n",
    "y_train = train_df[target_column].map({'Absence': 0, 'Presence': 1})\n",
    "X_test = test_df.drop('id', axis=1)\n",
    "test_ids = test_df['id'].values\n",
    "\n",
    "X_train = X_train.fillna(X_train.median()).reset_index(drop=True)\n",
    "X_test = X_test.fillna(X_test.median()).reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {X_train.shape} | Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c9cab",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_tr, y_tr, X_te, n_folds=5, seeds=[42, 123, 2024], tag=\"\"):\n",
    "    \"\"\"Train 9 models (3 seeds × 3 algorithms)\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    all_oof = []\n",
    "    all_test = []\n",
    "    all_scores = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        # XGBoost\n",
    "        oof_xgb = np.zeros(len(X_tr))\n",
    "        test_xgb = np.zeros(len(X_te))\n",
    "        sc = []\n",
    "        for fold, (tr_idx, val_idx) in enumerate(skf.split(X_tr, y_tr)):\n",
    "            print(f\"  [{tag}] XGB s={seed} fold={fold+1}\")\n",
    "            m = xgb.XGBClassifier(\n",
    "                n_estimators=5000, max_depth=5, learning_rate=0.005,\n",
    "                subsample=0.8, colsample_bytree=0.8, min_child_weight=5,\n",
    "                reg_alpha=0.1, reg_lambda=2.0, gamma=0.1,\n",
    "                random_state=seed, n_jobs=-1,\n",
    "                eval_metric='auc', early_stopping_rounds=200\n",
    "            )\n",
    "            m.fit(X_tr.iloc[tr_idx], y_tr.iloc[tr_idx],\n",
    "                  eval_set=[(X_tr.iloc[val_idx], y_tr.iloc[val_idx])],\n",
    "                  verbose=False)\n",
    "            oof_xgb[val_idx] = m.predict_proba(X_tr.iloc[val_idx])[:, 1]\n",
    "            test_xgb += m.predict_proba(X_te)[:, 1] / n_folds\n",
    "            sc.append(roc_auc_score(y_tr.iloc[val_idx], oof_xgb[val_idx]))\n",
    "        all_oof.append(oof_xgb)\n",
    "        all_test.append(test_xgb)\n",
    "        all_scores.append(sc)\n",
    "        print(f\"   XGB s={seed}: {np.mean(sc):.6f}\")\n",
    "\n",
    "        # LightGBM\n",
    "        oof_lgb = np.zeros(len(X_tr))\n",
    "        test_lgb = np.zeros(len(X_te))\n",
    "        sc = []\n",
    "        for fold, (tr_idx, val_idx) in enumerate(skf.split(X_tr, y_tr)):\n",
    "            print(f\"  [{tag}] LGB s={seed} fold={fold+1}\")\n",
    "            m = lgb.LGBMClassifier(\n",
    "                n_estimators=5000, max_depth=5, learning_rate=0.005,\n",
    "                subsample=0.8, colsample_bytree=0.8, min_child_samples=30,\n",
    "                reg_alpha=0.1, reg_lambda=2.0, num_leaves=31,\n",
    "                random_state=seed, n_jobs=-1, verbose=-1\n",
    "            )\n",
    "            m.fit(X_tr.iloc[tr_idx], y_tr.iloc[tr_idx],\n",
    "                  eval_set=[(X_tr.iloc[val_idx], y_tr.iloc[val_idx])],\n",
    "                  callbacks=[lgb.early_stopping(200, verbose=False),\n",
    "                              lgb.log_evaluation(-1)])\n",
    "            oof_lgb[val_idx] = m.predict_proba(X_tr.iloc[val_idx])[:, 1]\n",
    "            test_lgb += m.predict_proba(X_te)[:, 1] / n_folds\n",
    "            sc.append(roc_auc_score(y_tr.iloc[val_idx], oof_lgb[val_idx]))\n",
    "        all_oof.append(oof_lgb)\n",
    "        all_test.append(test_lgb)\n",
    "        all_scores.append(sc)\n",
    "        print(f\"   LGB s={seed}: {np.mean(sc):.6f}\")\n",
    "\n",
    "        # CatBoost\n",
    "        oof_cat = np.zeros(len(X_tr))\n",
    "        test_cat = np.zeros(len(X_te))\n",
    "        sc = []\n",
    "        for fold, (tr_idx, val_idx) in enumerate(skf.split(X_tr, y_tr)):\n",
    "            print(f\"  [{tag}] CAT s={seed} fold={fold+1}\")\n",
    "            m = CatBoostClassifier(\n",
    "                iterations=5000, depth=5, learning_rate=0.005,\n",
    "                l2_leaf_reg=5, random_state=seed, verbose=False,\n",
    "                thread_count=-1, early_stopping_rounds=200,\n",
    "                eval_metric='AUC'\n",
    "            )\n",
    "            m.fit(X_tr.iloc[tr_idx], y_tr.iloc[tr_idx],\n",
    "                  eval_set=(X_tr.iloc[val_idx], y_tr.iloc[val_idx]),\n",
    "                  verbose=False)\n",
    "            oof_cat[val_idx] = m.predict_proba(X_tr.iloc[val_idx])[:, 1]\n",
    "            test_cat += m.predict_proba(X_te)[:, 1] / n_folds\n",
    "            sc.append(roc_auc_score(y_tr.iloc[val_idx], oof_cat[val_idx]))\n",
    "        all_oof.append(oof_cat)\n",
    "        all_test.append(test_cat)\n",
    "        all_scores.append(sc)\n",
    "        print(f\"   CAT s={seed}: {np.mean(sc):.6f}\")\n",
    "\n",
    "    return all_oof, all_test, all_scores\n",
    "\n",
    "\n",
    "def best_ensemble(all_oof, all_test, y_tr):\n",
    "    \"\"\"Evaluate and select best ensemble method\"\"\"\n",
    "    simple_oof = np.mean(all_oof, axis=0)\n",
    "    simple_test = np.mean(all_test, axis=0)\n",
    "    simple_auc = roc_auc_score(y_tr, simple_oof)\n",
    "\n",
    "    def rank_avg(preds_list):\n",
    "        r = np.zeros(len(preds_list[0]))\n",
    "        for p in preds_list:\n",
    "            r += pd.Series(p).rank(pct=True).values\n",
    "        return r / len(preds_list)\n",
    "\n",
    "    rank_oof = rank_avg(all_oof)\n",
    "    rank_test = rank_avg(all_test)\n",
    "    rank_auc = roc_auc_score(y_tr, rank_oof)\n",
    "\n",
    "    meta = LogisticRegression(C=0.01, max_iter=1000, random_state=42)\n",
    "    mtr = pd.DataFrame({f'm{i}': all_oof[i] for i in range(len(all_oof))})\n",
    "    mte = pd.DataFrame({f'm{i}': all_test[i] for i in range(len(all_test))})\n",
    "    meta.fit(mtr, y_tr)\n",
    "    meta_oof = meta.predict_proba(mtr)[:, 1]\n",
    "    meta_test = meta.predict_proba(mte)[:, 1]\n",
    "    meta_auc = roc_auc_score(y_tr, meta_oof)\n",
    "\n",
    "    scores_dict = {\n",
    "        'simple': (simple_auc, simple_test),\n",
    "        'rank': (rank_auc, rank_test),\n",
    "        'meta': (meta_auc, meta_test),\n",
    "    }\n",
    "    best_name = max(scores_dict, key=lambda k: scores_dict[k][0])\n",
    "    best_auc, best_preds = scores_dict[best_name]\n",
    "    print(f\"  Simple={simple_auc:.6f} | Rank={rank_auc:.6f} | Meta={meta_auc:.6f}\")\n",
    "    print(f\"   Best: {best_name} ({best_auc:.6f})\")\n",
    "    return best_preds, best_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272efe9b",
   "metadata": {},
   "source": [
    "## ROUND 1: Initial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ROUND 1: Initial Training\")\n",
    "print(\"=\"*50)\n",
    "all_oof_r1, all_test_r1, scores_r1 = train_models(\n",
    "    X_train, y_train, X_test, tag=\"R1\")\n",
    "\n",
    "test_preds_r1, auc_r1 = best_ensemble(all_oof_r1, all_test_r1, y_train)\n",
    "print(f\"\\nRound 1 Best OOF AUC: {auc_r1:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2baf40e",
   "metadata": {},
   "source": [
    "## Pseudo-Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PSEUDO-LABELING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use high confidence threshold\n",
    "THRESHOLD = 0.05\n",
    "pseudo_mask = (test_preds_r1 < THRESHOLD) | (test_preds_r1 > (1 - THRESHOLD))\n",
    "pseudo_X = X_test[pseudo_mask].reset_index(drop=True)\n",
    "pseudo_y = pd.Series(\n",
    "    (test_preds_r1[pseudo_mask] > 0.5).astype(int)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"Pseudo-labeled samples: {pseudo_mask.sum()} / {len(X_test)}\")\n",
    "print(f\"  Pseudo 0 (Absence): {(pseudo_y == 0).sum()}\")\n",
    "print(f\"  Pseudo 1 (Presence): {(pseudo_y == 1).sum()}\")\n",
    "\n",
    "# Combine train + pseudo-labeled test\n",
    "X_train_r2 = pd.concat([X_train, pseudo_X], ignore_index=True)\n",
    "y_train_r2 = pd.concat([y_train, pseudo_y], ignore_index=True)\n",
    "\n",
    "print(f\"\\nRound 2 train size: {X_train_r2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84d429",
   "metadata": {},
   "source": [
    "## ROUND 2: Training with Pseudo-Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4599f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ROUND 2: Training with Pseudo-Labels\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "all_oof_r2, all_test_r2, scores_r2 = train_models(\n",
    "    X_train_r2, y_train_r2, X_test, tag=\"R2\")\n",
    "\n",
    "# Evaluate only on original train rows for fair comparison\n",
    "all_oof_r2_orig = [o[:len(X_train)] for o in all_oof_r2]\n",
    "test_preds_r2, auc_r2 = best_ensemble(\n",
    "    all_oof_r2_orig, all_test_r2, y_train)\n",
    "print(f\"\\nRound 2 Best OOF AUC: {auc_r2:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42f9a8",
   "metadata": {},
   "source": [
    "## Final Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13deff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL BLEND\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Try different blend ratios\n",
    "for w in [0.3, 0.4, 0.5]:\n",
    "    blend = w * test_preds_r1 + (1 - w) * test_preds_r2\n",
    "    print(f\"  Blend R1×{w:.1f} + R2×{1-w:.1f} — R1 AUC={auc_r1:.6f}, R2 AUC={auc_r2:.6f}\")\n",
    "\n",
    "# Use round with better OOF as primary\n",
    "if auc_r2 >= auc_r1:\n",
    "    final_test = 0.3 * test_preds_r1 + 0.7 * test_preds_r2\n",
    "    print(f\"\\n Using R2-dominant blend (R2 AUC higher: {auc_r2:.6f})\")\n",
    "else:\n",
    "    final_test = 0.7 * test_preds_r1 + 0.3 * test_preds_r2\n",
    "    print(f\"\\n Using R1-dominant blend (R1 AUC higher: {auc_r1:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0b50cf",
   "metadata": {},
   "source": [
    "## Save Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'Heart Disease': np.clip(final_test, 0, 1)\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission saved!\")\n",
    "print(submission.head(10))\n",
    "print(f\"Range: [{submission['Heart Disease'].min():.6f}, \"\n",
    "      f\"{submission['Heart Disease'].max():.6f}]\")\n",
    "print(\"File: submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
