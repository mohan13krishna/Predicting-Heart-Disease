{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6b5093",
   "metadata": {},
   "source": [
    "# ðŸ¥ Heart Disease Prediction - V17 Ensemble\n",
    "\n",
    "## **9-Model Ensemble with 3 Seeds + Blending Strategy**\n",
    "\n",
    "### Key Features:\n",
    "- **3 seeds per algorithm:** [42, 123, 2024] for maximum diversity\n",
    "- **9 models total:** LGB+CAT+XGB Ã— 3 seeds\n",
    "- **5-fold CV** with domain feature engineering\n",
    "- **Target encoding:** 7 categorical features (smoothing=30)\n",
    "- **Blend with V16:** Combines new 9-model ensemble with previous best (A.csv)\n",
    "- **Multiple blend ratios:** Tests 0.3, 0.4, 0.5, 0.6 weighting\n",
    "\n",
    "### Results:\n",
    "- **Best new ensemble AUC:** 0.955442 (meta-model)\n",
    "- **Kaggle Score:** 0.95360\n",
    "- **Status:** â­ Advanced blending strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c94c5",
   "metadata": {},
   "source": [
    "## Step 1: Load Data & Previous Best Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df  = pd.read_csv('test.csv')\n",
    "\n",
    "# Load previous best submission (V16)\n",
    "prev_sub = pd.read_csv('submission.csv')  # V16 submission\n",
    "prev_preds = prev_sub.set_index('id')['Heart Disease'].values\n",
    "print(f\"Previous submission loaded: {prev_sub.shape}\")\n",
    "print(f\"Previous submission range: [{prev_preds.min():.4f}, {prev_preds.max():.4f}]\")\n",
    "\n",
    "target_column = 'Heart Disease'\n",
    "X_train = train_df.drop([target_column, 'id'], axis=1)\n",
    "y_train = train_df[target_column].map({'Absence': 0, 'Presence': 1})\n",
    "X_test  = test_df.drop('id', axis=1)\n",
    "test_ids = test_df['id'].values\n",
    "\n",
    "X_train = X_train.fillna(X_train.median()).reset_index(drop=True)\n",
    "X_test  = X_test.fillna(X_test.median()).reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {X_train.shape} | Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090370b4",
   "metadata": {},
   "source": [
    "## Step 2: Domain Feature Engineering + Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    df['cp_asymptomatic']  = (df['Chest pain type'] == 4).astype(int)\n",
    "    df['st_significant']   = (df['ST depression'] > 2.0).astype(int)\n",
    "    df['max_hr_ratio']     = df['Max HR'] / (220 - df['Age'])\n",
    "    df['hr_reserve']       = (220 - df['Age']) - df['Max HR']\n",
    "    df['has_vessels']      = (df['Number of vessels fluro'] > 0).astype(int)\n",
    "    df['thal_reversible']  = (df['Thallium'] == 7).astype(int)\n",
    "    df['thal_fixed']       = (df['Thallium'] == 6).astype(int)\n",
    "    df['angina_st']        = df['Exercise angina'] * df['ST depression']\n",
    "    df['cp_angina']        = df['Chest pain type'] * df['Exercise angina']\n",
    "    df['age_male']         = df['Age'] * df['Sex']\n",
    "    df['vessels_thal']     = df['Number of vessels fluro'] * df['Thallium']\n",
    "    df['st_slope_combo']   = df['ST depression'] * df['Slope of ST']\n",
    "    df['risk_score']       = (\n",
    "        df['cp_asymptomatic'] +\n",
    "        df['st_significant'] +\n",
    "        df['has_vessels'] +\n",
    "        df['thal_reversible'] +\n",
    "        df['Exercise angina'] +\n",
    "        (df['ST depression'] > 1.0).astype(int) +\n",
    "        (df['max_hr_ratio'] < 0.7).astype(int)\n",
    "    )\n",
    "    df['age_group'] = pd.cut(df['Age'], bins=[0,45,55,65,100], labels=[0,1,2,3]).astype(int)\n",
    "    df['high_bp']   = (df['BP'] > 140).astype(int)\n",
    "    df['high_chol'] = (df['Cholesterol'] > 240).astype(int)\n",
    "    df['fbs_age']   = df['FBS over 120'] * df['Age']\n",
    "    return df\n",
    "\n",
    "X_train = engineer_features(X_train)\n",
    "X_test  = engineer_features(X_test)\n",
    "\n",
    "TE_COLS = ['Chest pain type', 'Thallium', 'Slope of ST',\n",
    "           'Number of vessels fluro', 'EKG results',\n",
    "           'risk_score', 'age_group']\n",
    "\n",
    "def target_encode(X_tr, y_tr, X_val, X_te, cols, smoothing=30):\n",
    "    gm = y_tr.mean()\n",
    "    X_tr_e, X_val_e, X_te_e = X_tr.copy(), X_val.copy(), X_te.copy()\n",
    "    for col in cols:\n",
    "        stats = y_tr.groupby(X_tr[col]).agg(['sum','count'])\n",
    "        sm = (stats['sum'] + gm * smoothing) / (stats['count'] + smoothing)\n",
    "        X_tr_e[f'{col}_te']  = X_tr[col].map(sm).fillna(gm)\n",
    "        X_val_e[f'{col}_te'] = X_val[col].map(sm).fillna(gm)\n",
    "        X_te_e[f'{col}_te']  = X_te[col].map(sm).fillna(gm)\n",
    "    return X_tr_e, X_val_e, X_te_e\n",
    "\n",
    "print(f\"Features engineered and ready for encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daa8f31",
   "metadata": {},
   "source": [
    "## Step 3: Train 9 Models with 3 Seeds (LGB+CAT+XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "SEEDS   = [42, 123, 2024]  # 3 seeds for more diversity\n",
    "skf     = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "all_oof   = []\n",
    "all_test  = []\n",
    "all_scores = []\n",
    "\n",
    "print(f\"Training 9 models Ã— {n_folds} folds\")\n",
    "print(f\"Est. runtime: ~8 hours\\n\")\n",
    "\n",
    "for seed in SEEDS:\n",
    "\n",
    "    # --- LightGBM ---\n",
    "    oof_lgb  = np.zeros(len(X_train))\n",
    "    test_lgb = np.zeros(len(X_test))\n",
    "    sc_lgb   = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        X_tr_e, X_val_e, X_te_e = target_encode(\n",
    "            X_train.iloc[tr_idx], y_train.iloc[tr_idx],\n",
    "            X_train.iloc[val_idx], X_test, TE_COLS)\n",
    "\n",
    "        m = lgb.LGBMClassifier(\n",
    "            n_estimators=10000, learning_rate=0.01, subsample=0.7,\n",
    "            colsample_bytree=0.7, min_child_samples=50, reg_alpha=0.1,\n",
    "            reg_lambda=5.0, num_leaves=31, random_state=seed, n_jobs=-1, verbose=-1\n",
    "        )\n",
    "        m.fit(X_tr_e, y_train.iloc[tr_idx],\n",
    "              eval_set=[(X_val_e, y_train.iloc[val_idx])],\n",
    "              callbacks=[lgb.early_stopping(200, verbose=False),\n",
    "                         lgb.log_evaluation(-1)])\n",
    "        oof_lgb[val_idx] = m.predict_proba(X_val_e)[:, 1]\n",
    "        test_lgb += m.predict_proba(X_te_e)[:, 1] / n_folds\n",
    "        sc_lgb.append(roc_auc_score(y_train.iloc[val_idx], oof_lgb[val_idx]))\n",
    "\n",
    "    all_oof.append(oof_lgb)\n",
    "    all_test.append(test_lgb)\n",
    "    all_scores.append(sc_lgb)\n",
    "    print(f\"LGB-s{seed}: {np.mean(sc_lgb):.6f}\")\n",
    "\n",
    "    # --- CatBoost ---\n",
    "    oof_cat  = np.zeros(len(X_train))\n",
    "    test_cat = np.zeros(len(X_test))\n",
    "    sc_cat   = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        X_tr_e, X_val_e, X_te_e = target_encode(\n",
    "            X_train.iloc[tr_idx], y_train.iloc[tr_idx],\n",
    "            X_train.iloc[val_idx], X_test, TE_COLS)\n",
    "\n",
    "        m = CatBoostClassifier(\n",
    "            iterations=10000, depth=5, learning_rate=0.01, l2_leaf_reg=10,\n",
    "            min_data_in_leaf=50, random_state=seed, verbose=False,\n",
    "            thread_count=-1, early_stopping_rounds=200, eval_metric='AUC'\n",
    "        )\n",
    "        m.fit(X_tr_e, y_train.iloc[tr_idx],\n",
    "              eval_set=(X_val_e, y_train.iloc[val_idx]), verbose=False)\n",
    "        oof_cat[val_idx] = m.predict_proba(X_val_e)[:, 1]\n",
    "        test_cat += m.predict_proba(X_te_e)[:, 1] / n_folds\n",
    "        sc_cat.append(roc_auc_score(y_train.iloc[val_idx], oof_cat[val_idx]))\n",
    "\n",
    "    all_oof.append(oof_cat)\n",
    "    all_test.append(test_cat)\n",
    "    all_scores.append(sc_cat)\n",
    "    print(f\"CAT-s{seed}: {np.mean(sc_cat):.6f}\")\n",
    "\n",
    "    # --- XGBoost ---\n",
    "    oof_xgb  = np.zeros(len(X_train))\n",
    "    test_xgb = np.zeros(len(X_test))\n",
    "    sc_xgb   = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        X_tr_e, X_val_e, X_te_e = target_encode(\n",
    "            X_train.iloc[tr_idx], y_train.iloc[tr_idx],\n",
    "            X_train.iloc[val_idx], X_test, TE_COLS)\n",
    "\n",
    "        m = xgb.XGBClassifier(\n",
    "            n_estimators=10000, max_depth=4, learning_rate=0.01, subsample=0.7,\n",
    "            colsample_bytree=0.7, min_child_weight=10, reg_alpha=0.1,\n",
    "            reg_lambda=5.0, gamma=0.2, random_state=seed, n_jobs=-1,\n",
    "            eval_metric='auc', early_stopping_rounds=200\n",
    "        )\n",
    "        m.fit(X_tr_e, y_train.iloc[tr_idx],\n",
    "              eval_set=[(X_val_e, y_train.iloc[val_idx])], verbose=False)\n",
    "        oof_xgb[val_idx] = m.predict_proba(X_val_e)[:, 1]\n",
    "        test_xgb += m.predict_proba(X_te_e)[:, 1] / n_folds\n",
    "        sc_xgb.append(roc_auc_score(y_train.iloc[val_idx], oof_xgb[val_idx]))\n",
    "\n",
    "    all_oof.append(oof_xgb)\n",
    "    all_test.append(test_xgb)\n",
    "    all_scores.append(sc_xgb)\n",
    "    print(f\"XGB-s{seed}: {np.mean(sc_xgb):.6f}\\n\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097aacf",
   "metadata": {},
   "source": [
    "## Step 4: Ensemble Methods & Blending with V16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e095841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ensemble methods\n",
    "def rank_avg(preds_list):\n",
    "    r = np.zeros(len(preds_list[0]))\n",
    "    for p in preds_list:\n",
    "        r += pd.Series(p).rank(pct=True).values\n",
    "    return r / len(preds_list)\n",
    "\n",
    "simple_oof  = np.mean(all_oof, axis=0)\n",
    "simple_test = np.mean(all_test, axis=0)\n",
    "simple_auc  = roc_auc_score(y_train, simple_oof)\n",
    "\n",
    "rank_oof  = rank_avg(all_oof)\n",
    "rank_test = rank_avg(all_test)\n",
    "rank_auc  = roc_auc_score(y_train, rank_oof)\n",
    "\n",
    "meta = LogisticRegression(C=0.01, max_iter=1000, random_state=42)\n",
    "mtr  = pd.DataFrame({f'm{i}': all_oof[i]  for i in range(len(all_oof))})\n",
    "mte  = pd.DataFrame({f'm{i}': all_test[i] for i in range(len(all_test))})\n",
    "meta.fit(mtr, y_train)\n",
    "meta_oof  = meta.predict_proba(mtr)[:, 1]\n",
    "meta_test = meta.predict_proba(mte)[:, 1]\n",
    "meta_auc  = roc_auc_score(y_train, meta_oof)\n",
    "\n",
    "print(f\"Simple Average AUC: {simple_auc:.6f}\")\n",
    "print(f\"Rank Average AUC:   {rank_auc:.6f}\")\n",
    "print(f\"Meta-model AUC:     {meta_auc:.6f}\")\n",
    "\n",
    "# Select best\n",
    "scores_dict = {\n",
    "    'simple': (simple_auc, simple_test),\n",
    "    'rank':   (rank_auc,   rank_test),\n",
    "    'meta':   (meta_auc,   meta_test),\n",
    "}\n",
    "best_name = max(scores_dict, key=lambda k: scores_dict[k][0])\n",
    "best_auc, new_preds = scores_dict[best_name]\n",
    "print(f\"\\nâœ… Best ensemble: {best_name.upper()} (AUC: {best_auc:.6f})\")\n",
    "\n",
    "# Blend with V16\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BLENDING with V16 (A.csv):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def rank_norm(preds):\n",
    "    return pd.Series(preds).rank(pct=True).values\n",
    "\n",
    "new_preds_rn  = rank_norm(new_preds)\n",
    "prev_preds_rn = rank_norm(prev_preds)\n",
    "\n",
    "# Test multiple blend ratios\n",
    "for w_prev in [0.3, 0.4, 0.5, 0.6]:\n",
    "    w_new  = 1.0 - w_prev\n",
    "    blend  = w_prev * prev_preds_rn + w_new * new_preds_rn\n",
    "    sub = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'Heart Disease': np.clip(blend, 0, 1)\n",
    "    })\n",
    "    sub.to_csv(f'blend_{w_prev}.csv', index=False)\n",
    "    print(f\"Saved blend_{w_prev}.csv\")\n",
    "\n",
    "# Main submission: 50/50 blend\n",
    "final_blend = 0.5 * prev_preds_rn + 0.5 * new_preds_rn\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'Heart Disease': np.clip(final_blend, 0, 1)\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ… Main submission saved (50/50 blend)\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
